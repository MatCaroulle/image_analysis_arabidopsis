{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosette phenotyping\n",
    "\n",
    "use conda activate PhenoLeaf to set the container environment active"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image corrections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image optical-distortion correction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images of full tray displays some defects whixh may be linked to optical distortion.\n",
    "It is observed a barrel type of radial distortion i.e. plants at the center of the tray tends to be bigger than they are and conversly plants at the edges of the tray tends to be small. In addition these size defects that can be easyly observed on the Fig 0. Optical distortions also tiggers modifications in shapes and sizes of image's pixels. The causes of image optical-distortion can be various (due to inegal lens magnification, camera sensor, non parrallel plane-object)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![texte alternatif](/home/mcaroulle/Pheno_Leaf/Image optical-distortion.png \"titre\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus it is necessary to fix these kinds of optical defects. We use an open-source python package named \"Discorpy\" wich github repository can be found here https://github.com/algotom/discorpy/tree/master\n",
    "\n",
    "This package use a 3 step process:\n",
    "- pre-processing : extracting and grouping reference-points from a calibration image\n",
    "- processing : caculation of coefficients of correction for the model\n",
    "- post-processing : correcting images\n",
    "\n",
    "\n",
    "We used to calibrate the commercial camera used (Canon EOS800D) using a chessboard image.\n",
    "The chessboard pattern was generated using following website:\n",
    "https://markhedleyjones.com/projects/calibration-checkerboard-collection\n",
    "(4 A4 chesscheckboards were stuck together and used a a single calibration checkboard).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal slope:  -0.002617999859155802  Distance:  200.35972055719603\n",
      "Vertical slope:  -0.005236035605700117  Distance:  199.7484348376889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import discorpy\n",
    "import discorpy.losa.loadersaver as io\n",
    "import discorpy.prep.preprocessing as prep\n",
    "import discorpy.prep.linepattern as lprep\n",
    "import discorpy.proc.processing as proc\n",
    "import discorpy.post.postprocessing as post\n",
    "\n",
    "# Initial parameters\n",
    "file_path = \"/home/mcaroulle/Pheno_Leaf/PL_Final/RAW_INPUT/Real-chessboard.JPG\" #import the chessboradcheck image\n",
    "output_base = \"/home/mcaroulle/Pheno_Leaf/PL_Final/RAW_OUTPUT\"\n",
    "num_coef = 5  # Number of polynomial coefficients\n",
    "mat0 = io.load_image(file_path) # Load image\n",
    "(height, width) = mat0.shape\n",
    "\n",
    "# Convert the chessboard image to a line-pattern image\n",
    "mat1 = lprep.convert_chessboard_to_linepattern(mat0)\n",
    "io.save_image(output_base + \"/line_pattern_converted.jpg\", mat1)\n",
    "\n",
    "# Calculate slope and distance between lines\n",
    "slope_hor, dist_hor = lprep.calc_slope_distance_hor_lines(mat1, radius=15, sensitive=0.5)\n",
    "slope_ver, dist_ver = lprep.calc_slope_distance_ver_lines(mat1, radius=15, sensitive=0.5)\n",
    "print(\"Horizontal slope: \", slope_hor, \" Distance: \", dist_hor)\n",
    "print(\"Vertical slope: \", slope_ver, \" Distance: \", dist_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mcaroulle/Pheno_Leaf/PL_Final/RAW_OUTPUT/ver_residual_before_correction.png'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract reference-points\n",
    "list_points_hor_lines = lprep.get_cross_points_hor_lines(mat1, slope_ver, dist_ver,\n",
    "                                                         ratio=0.3, norm=True, offset=450,\n",
    "                                                         bgr=\"bright\", radius=15,\n",
    "                                                         sensitive=0.5, denoise=True,\n",
    "                                                         subpixel=True)\n",
    "list_points_ver_lines = lprep.get_cross_points_ver_lines(mat1, slope_hor, dist_hor,\n",
    "                                                         ratio=0.3, norm=True, offset=150,\n",
    "                                                         bgr=\"bright\", radius=15,\n",
    "                                                         sensitive=0.5, denoise=True,\n",
    "                                                         subpixel=True)\n",
    "if len(list_points_hor_lines) == 0 or len(list_points_ver_lines) == 0:\n",
    "    raise ValueError(\"No reference-points detected !!! Please adjust parameters !!!\")\n",
    "io.save_plot_points(output_base + \"/ref_points_horizontal.png\", list_points_hor_lines,\n",
    "                    height, width, color=\"red\")\n",
    "io.save_plot_points(output_base + \"/ref_points_vertical.png\", list_points_ver_lines,\n",
    "                    height, width, color=\"blue\")\n",
    "\n",
    "# Group points into lines\n",
    "list_hor_lines = prep.group_dots_hor_lines(list_points_hor_lines, slope_hor, dist_hor,\n",
    "                                           ratio=0.1, num_dot_miss=2, accepted_ratio=0.8)\n",
    "list_ver_lines = prep.group_dots_ver_lines(list_points_ver_lines, slope_ver, dist_ver,\n",
    "                                           ratio=0.1, num_dot_miss=2, accepted_ratio=0.8)\n",
    "\n",
    "# Remove residual dots\n",
    "list_hor_lines = prep.remove_residual_dots_hor(list_hor_lines, slope_hor, 2.0)\n",
    "list_ver_lines = prep.remove_residual_dots_ver(list_ver_lines, slope_ver, 2.0)\n",
    "\n",
    "# Save output for checking\n",
    "io.save_plot_image(output_base + \"/horizontal_lines.png\", list_hor_lines, height, width)\n",
    "io.save_plot_image(output_base + \"/vertical_lines.png\", list_ver_lines, height, width)\n",
    "list_hor_data = post.calc_residual_hor(list_hor_lines, 0.0, 0.0)\n",
    "list_ver_data = post.calc_residual_ver(list_ver_lines, 0.0, 0.0)\n",
    "io.save_residual_plot(output_base + \"/hor_residual_before_correction.png\",\n",
    "                      list_hor_data, height, width)\n",
    "io.save_residual_plot(output_base + \"/ver_residual_before_correction.png\",\n",
    "                      list_ver_data, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-center: 2681.444742082234. Y-center: 1983.8462758586431\n",
      "Coefficients: [ 1.00840054e+00 -1.03875450e-05 -6.37773587e-09  1.28491195e-12\n",
      " -1.52520141e-16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/mcaroulle/Pheno_Leaf/PL_Final/RAW_OUTPUT/ver_residual_after_correction.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regenerate grid points after correcting the perspective effect.\n",
    "list_hor_lines, list_ver_lines = proc.regenerate_grid_points_parabola(\n",
    "    list_hor_lines, list_ver_lines, perspective=True)\n",
    "\n",
    "# Calculate parameters of the radial correction model\n",
    "(xcenter, ycenter) = proc.find_cod_coarse(list_hor_lines, list_ver_lines)\n",
    "list_fact = proc.calc_coef_backward(list_hor_lines, list_ver_lines,\n",
    "                                    xcenter, ycenter, num_coef)\n",
    "io.save_metadata_txt(output_base + \"/coefficients_radial_distortion.txt\",\n",
    "                     xcenter, ycenter, list_fact)\n",
    "\n",
    "print(\"X-center: {0}. Y-center: {1}\".format(xcenter, ycenter))\n",
    "print(\"Coefficients: {0}\".format(list_fact))\n",
    "\n",
    "# Check the correction results:\n",
    "# Apply correction to the lines of points\n",
    "list_uhor_lines = post.unwarp_line_backward(list_hor_lines, xcenter, ycenter,\n",
    "                                            list_fact)\n",
    "list_uver_lines = post.unwarp_line_backward(list_ver_lines, xcenter, ycenter,\n",
    "                                            list_fact)\n",
    "# Calculate the residual of the unwarpped points.\n",
    "list_hor_data = post.calc_residual_hor(list_uhor_lines, xcenter, ycenter)\n",
    "list_ver_data = post.calc_residual_ver(list_uver_lines, xcenter, ycenter)\n",
    "# Save the results for checking\n",
    "io.save_plot_image(output_base + \"/unwarpped_horizontal_lines.png\",\n",
    "                   list_uhor_lines, height, width)\n",
    "io.save_plot_image(output_base + \"/unwarpped_vertical_lines.png\",\n",
    "                   list_uver_lines, height, width)\n",
    "io.save_residual_plot(output_base + \"/hor_residual_after_correction.png\",\n",
    "                      list_hor_data, height, width)\n",
    "io.save_residual_plot(output_base + \"/ver_residual_after_correction.png\",\n",
    "                      list_ver_data, height, width)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the coefficents corrections are calculated, we can apply them to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load coefficients from previous calculation\n",
    "(xcenter, ycenter, list_fact) = io.load_metadata_txt(\n",
    "    output_base + \"/coefficients_radial_distortion.txt\")\n",
    "\n",
    "# Load an image and correct it.\n",
    "img = io.load_image(\"/home/mcaroulle/Pheno_Leaf/test.JPG\", average=False)\n",
    "img_corrected = np.copy(img)\n",
    "for i in range(img.shape[-1]):\n",
    "    img_corrected[:, :, i] = post.unwarp_image_backward(img[:, :, i], xcenter,\n",
    "                                                        ycenter, list_fact)\n",
    "io.save_image(output_base + \"/TRUE_image_corrected.jpg\", img_corrected)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve natural python thresholding commands it is better to homogenize the color of our images likewise the pipeline could be applied to all the dataset minimizing errors due to color differences and allowing to set the same thresholding parameters for all the images.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our images displays a large variety of colors and texturess which can make more difficult this color uniformisation by simple color modifications.\n",
    "\n",
    "Here we propose to use a pipeline designed in the following study to perform color normalisation using machine learning and a method described in https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#Set input directory\n",
    "input_dir=\"/home/mcaroulle/Pheno_Leaf/Data/Color normalization/Input/\"\n",
    "input_image_list=os.listdir(input_dir)\n",
    "\n",
    "#Set output directory\n",
    "output_dir=\"/home/mcaroulle/Pheno_Leaf/Data/Color normalization/Output/\"\n",
    "\n",
    "def get_mean_and_std(x):\n",
    "\tx_mean, x_std = cv2.meanStdDev(x)\n",
    "\tx_mean = np.hstack(np.around(x_mean,2))\n",
    "\tx_std = np.hstack(np.around(x_std,2))\n",
    "\treturn x_mean, x_std\n",
    "\n",
    "#Set input for the template library\n",
    "template_img = cv2.imread('/home/mcaroulle/Pheno_Leaf/Data/Color normalization/Template/1.png')\n",
    "template_img = cv2.cvtColor(template_img,cv2.COLOR_BGR2LAB)\n",
    "template_mean, template_std = get_mean_and_std(template_img)\n",
    "\n",
    "for img in (input_image_list):\n",
    "    print(img)\n",
    "    input_img = cv2.imread(input_dir+img)\n",
    "    input_img = cv2.cvtColor(input_img,cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    \n",
    "    img_mean, img_std = get_mean_and_std(input_img)\n",
    "    \n",
    "    \n",
    "    height, width, channel = input_img.shape\n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width):\n",
    "            for k in range(0,channel):\n",
    "                x = input_img[i,j,k]\n",
    "                x = ((x-img_mean[k])*(template_std[k]/img_std[k]))+template_mean[k]\n",
    "                x = round(x)\n",
    "                x = 0 if x<0 else x\n",
    "                x = 255 if x>255 else x\n",
    "                input_img[i,j,k] = x\n",
    "            \n",
    "    input_img= cv2.cvtColor(input_img,cv2.COLOR_LAB2BGR)\n",
    "    cv2.imwrite(output_dir+\"modified_\"+img, input_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's clean the noise using edge preserving filter.\n",
    "#As mentioned in previous tutorial, my favorite is NLM\n",
    "\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage import img_as_ubyte, img_as_float\n",
    "\n",
    "float_img = img_as_float(img)\n",
    "sigma_est = np.mean(estimate_sigma(float_img, multichannel=True))\n",
    "\n",
    "\n",
    "denoise_img = denoise_nl_means(float_img, h=1.15 * sigma_est, fast_mode=False, \n",
    "                               patch_size=5, patch_distance=3, multichannel=True)\n",
    "                           \n",
    "denoise_img_as_8byte = img_as_ubyte(denoise_img)\n",
    "#plt.imshow(denoise_img_as_8byte, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color segmentation using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lybraries importation\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io, measure\n",
    "\n",
    "#Image path\n",
    "img=io.imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/13.png')\n",
    "#plt.imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##convert to hsv\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "mask= cv2.inRange(hsv,(30,20,150),(70,255,255))\n",
    "\n",
    "#plt.imshow(mask)\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "closed_mask = nd. binary_closing(mask, np. ones ((7,7)))\n",
    "plt.imshow(closed_mask)\n",
    "\n",
    "label_image= measure.label(closed_mask)\n",
    "#plt.imshow(label_image)\n",
    "\n",
    "from skimage.color import label2rgb\n",
    "image_label_overlay = label2rgb(label_image, image= img)\n",
    "#plt.imshow(image_label_overlay)\n",
    "\n",
    "props = measure.regionprops_table(label_image, img, \n",
    "                                  properties=['label','area','equivalent_diameter'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(props)\n",
    "#print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importation\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io, measure\n",
    "\n",
    "# Image path\n",
    "img = io.imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/13.png')\n",
    "\n",
    "## Convert to hsv\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "mask = cv2.inRange(hsv, (30, 20, 150), (70, 255, 255))\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "closed_mask = nd.binary_closing(mask, np.ones((7, 7)))\n",
    "\n",
    "label_image = measure.label(closed_mask)\n",
    "\n",
    "props = measure.regionprops_table(label_image, img,\n",
    "                                  properties=['label', 'area', 'centroid'])\n",
    "\n",
    "# Convert props to DataFrame\n",
    "df = pd.DataFrame(props)\n",
    "\n",
    "# Filter regions with area less than 5,000\n",
    "df_filtered = df[df['area'] >= 5000]\n",
    "print(df_filtered)\n",
    "\n",
    "# Filter label_image based on the filtered DataFrame\n",
    "filtered_label_image = np.isin(label_image, df_filtered['label'])\n",
    "\n",
    "# Show the filtered label_image\n",
    "plt.imshow(filtered_label_image)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function wich fuse all the labels contained in a list\n",
    "\n",
    "def merge_labels(label_image, df):\n",
    "    list_labels = df['label'].tolist()\n",
    "\n",
    "    for k in range (len(list_labels)-1):\n",
    "        merged_label_image = np.copy(label_image)\n",
    "        merged_label_image[merged_label_image == list_labels[k+1]] = list_labels[k]\n",
    "\n",
    "    return merged_label_image\n",
    "\n",
    "\n",
    "\n",
    "merged_label_image = merge_labels(filtered_label_image, df_filtered)\n",
    "plt.imshow(merged_label_image)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image skeletonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import skeletonize\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import invert\n",
    "\n",
    "# Invert the horse image\n",
    "image = merged_label_image\n",
    "\n",
    "# perform skeletonization\n",
    "skeleton = skeletonize(image)\n",
    "\n",
    "# display results\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4),\n",
    "                         sharex=True, sharey=True)\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('original', fontsize=20)\n",
    "\n",
    "ax[1].imshow(skeleton, cmap=plt.cm.gray)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('skeleton', fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "image = merged_label_image\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Overlapping objects')\n",
    "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Distances')\n",
    "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
    "ax[2].set_title('Separated objects')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import data,io,measure\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "#Image path\n",
    "image=io.imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/13.png')\n",
    "\n",
    "\n",
    "##convert to hsv\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "mask= cv2.inRange(hsv,(30,20,150),(70,255,255))\n",
    "\n",
    "closed_mask = nd. binary_closing(mask, np. ones ((7,7)))\n",
    "\n",
    "\n",
    "label_image= measure.label(closed_mask)\n",
    "\n",
    "cleared = clear_border(label_image)\n",
    "\n",
    "# label image regions\n",
    "label_image = label(cleared)\n",
    "# to make the background transparent, pass the value of `bg_label`,\n",
    "# and leave `bg_color` as `None` and `kind` as `overlay`\n",
    "image_label_overlay = label2rgb(label_image, image=image, bg_label=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.imshow(image_label_overlay)\n",
    "\n",
    "for region in regionprops(label_image):\n",
    "    # take regions with large enough areas\n",
    "    if region.area >= 5000:\n",
    "        # draw rectangle around segmented coins\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                  fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## whole tray segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lybraries importation\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io, measure\n",
    "from scipy import ndimage as nd\n",
    "from skimage.color import label2rgb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Image path\n",
    "img=io.imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/TRAY_1.png')\n",
    "#plt.imshow(img)\n",
    "\n",
    "##convert to hsv\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "mask= cv2.inRange(hsv,(30,20,150),(70,255,255))\n",
    "\n",
    "closed_mask = nd.binary_closing(mask, np. ones ((7,7)))\n",
    "\n",
    "\n",
    "label_image= measure.label(closed_mask)\n",
    "\n",
    "\n",
    "\n",
    "image_label_overlay = label2rgb(label_image, image= img)\n",
    "plt.imshow(image_label_overlay)\n",
    "\n",
    "props = measure.regionprops_table(label_image, img, \n",
    "                                  properties=['label','area','centroid','coords'])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(props)\n",
    "#print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove border artefacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "\n",
    "# remove artifacts connected to image border\n",
    "cleared_label_image = segmentation.clear_border(label_image)\n",
    "\n",
    "#take measurements \n",
    "props = measure.regionprops_table(cleared_label_image, img,\n",
    "                                  properties=['label', 'area', 'centroid','coords'])\n",
    "\n",
    "# Convert props to DataFrame\n",
    "df = pd.DataFrame(props)\n",
    "\n",
    "plt.imshow(cleared_label_image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter regions with area less than 5,000\n",
    "df_filtered = df[df['area'] >= 500]\n",
    "\n",
    "\n",
    "# Filter label_image based on the filtered DataFrame\n",
    "filtered_label_image = np.isin(cleared_label_image , df_filtered['label'])\n",
    "\n",
    "# Show the filtered label_image\n",
    "plt.imshow(filtered_label_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroids plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "\n",
    "# Charger l'image\n",
    "image = filtered_label_image\n",
    "\n",
    "# Créer la figure et les axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Afficher l'image\n",
    "ax.imshow(image)\n",
    "\n",
    "centrioles_x = df_filtered['centroid-0'].tolist()\n",
    "centrioles_y = df_filtered['centroid-1'].tolist()\n",
    "\n",
    "# Tracer les centrioles\n",
    "for x, y in zip(centrioles_x, centrioles_y):\n",
    "    circle = patches.Circle((x, y), radius=5, color='red')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "# Afficher le tracé\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tray Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "\n",
    "# Charger l'image\n",
    "image = imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/TRAY_1.png')\n",
    "\n",
    "# Extraire les dimensions de l'image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "#Calcul de la largeur et de la hauteur des pots dans le tray\n",
    "x_pot = width/7\n",
    "y_pot = height/5\n",
    "\n",
    "\n",
    "\n",
    "# Création des données qui correspond à un découpage du tray en pots\n",
    "x_min=[]\n",
    "x_max=[]\n",
    "X=[]\n",
    "\n",
    "for k in range(5):\n",
    "    x=0\n",
    "    for k in range (8):\n",
    "        X.append(int(x))\n",
    "        x+=x_pot\n",
    "    x_min+=X[0:7]\n",
    "    x_max+=X[1:8]\n",
    "    if x_max[-1]!=width:\n",
    "        x_max[-1]=width\n",
    "\n",
    "\n",
    "y_min=[]\n",
    "y_max=[]\n",
    "Y=[]\n",
    "\n",
    "\n",
    "for k in range(7):\n",
    "    y=0\n",
    "    for k in range (6):\n",
    "        Y.append(int(y))\n",
    "        y+=y_pot\n",
    "    y_min+=Y[0:5]\n",
    "    y_max+=Y[1:6]\n",
    "    if y_max[-1]!=height:\n",
    "        y_max[-1]=height\n",
    "\n",
    "\n",
    "data_tray_max = {'Pot_position': [i for i in range(1, 36)],\n",
    "        'xmin': [k for k in x_min],\n",
    "        'xmax': [k for k in x_max],\n",
    "        'ymin': [k for k in y_min],\n",
    "        'ymax': [k for k in y_max]}\n",
    "\n",
    "df_tray_max = pd.DataFrame(data_tray_max)\n",
    "#print(df_tray_max)\n",
    "\n",
    "\n",
    "##Opération sur df_tray_max pour créer df_tray_min\n",
    "\n",
    "#Calculation of the rejected zone\n",
    "x_R_pot = int((width/7)/20)\n",
    "y_R_pot = int((height/5)/20)\n",
    "\n",
    "\n",
    "df_tray_min = pd.DataFrame(data_tray_max)\n",
    "\n",
    "\n",
    "df_tray_min['xmin'] = df_tray_min['xmin'] + x_R_pot\n",
    "df_tray_min['xmax'] = df_tray_min['xmax'] - x_R_pot\n",
    "df_tray_min['ymin'] = df_tray_min['ymin'] + y_R_pot\n",
    "df_tray_min['ymax'] = df_tray_min['ymax'] - y_R_pot\n",
    "\n",
    "##création du tableau ne contenant que les centroïdes en filtrant le dataframe du masque obtenu précemment\n",
    "df_centroid = df_filtered [['label','centroid-0','centroid-1']]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BROUILLON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée un df vide pour recueillir les résultats\n",
    "data_resultat = {'label': [],\n",
    "        'centroid-0': [],\n",
    "        'centroid-1': []}\n",
    "\n",
    "df_resultat = pd.DataFrame(data_resultat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "for k in range (len(df_tray_max)):\n",
    "\n",
    "    ##### CENTROIDES PRESENTS DANS TRAY MAX\n",
    "\n",
    "    df_subset_centroid_max= df_centroid[(df_centroid['centroid-0'] >= df_tray_max['xmin'][k]) & (df_centroid['centroid-0'] <= df_tray_max['xmax'][k]) & (df_centroid['centroid-1'] >= df_tray_max['ymin'][k]) & (df_centroid['centroid-1'] <= df_tray_max['ymax'][k])]\n",
    "\n",
    "\n",
    "    #### CENTROIDES PRESENTS DANS LE TRAY MIN\n",
    "\n",
    "    # Filtrer les points dans le DataFrame 1 en fonction des coordonnées\n",
    "    df_subset_centroid_min= df_centroid[(df_centroid['centroid-0'] >= dataframe2['xmin'][k]) & (df_centroid['centroid-0'] <= dataframe2['xmax'][k]) & (df_centroid['centroid-1'] >= dataframe2['ymin'][k]) & (df_centroid['centroid-1'] <= dataframe2['ymax'][k])]\n",
    "\n",
    "\n",
    "\n",
    "    ####CENTROIDES PRESENTS DANS LA ZONE D'EXCLUSION\n",
    "\n",
    "    df_subset_centroid_rejected = df_subset_centroid_max[~df_subset_centroid_max['label'].isin(df_subset_centroid_min['label'])]\n",
    "\n",
    "    df_resultat= pd.concat([df_resultat,df_subset_centroid_rejected], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###ON SUPPRIME LES OBJETS SITUES DANS LA ZONE D4EXCLUSION\n",
    "#resultat += df_centroid[~df_centroid['label'].isin(df_subset_centroid_rejected['label'])]\n",
    "\n",
    "\n",
    "##print(df_subset_centroid_rejected)\n",
    "\n",
    "#df_final = df_filtered[~df_filtered['label'].isin(df_subset_centroid_rejected['label'])]\n",
    "\n",
    "# Affichage\n",
    "#on_y_est = np.isin(cleared_label_image , df_final['label'])\n",
    "\n",
    "\n",
    "# Show the filtered label_image\n",
    "#plt.imshow(on_y_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création des DataFrames exemple\n",
    "data1 = {'Colonne1': ['A', 'B', 'C', 'D']}\n",
    "data2 = {'Colonne1': ['B', 'D', 'E', 'F']}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Filtrage des lignes\n",
    "df_filtered = df1MAX[~df1MAX['Colonne1'].isin(df2MIN['Colonne1'])]\n",
    "\n",
    "# Affichage du DataFrame filtré\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##### CENTROIDES PRESENTS DANS TRAY MAX\n",
    "\n",
    "# Exemple de DataFrame 1\n",
    "dataframe1 = df_centroid\n",
    "\n",
    "# Exemple de DataFrame 2\n",
    "dataframe2 = df_tray_max\n",
    "\n",
    "# Récupérer les valeurs de la première ligne du DataFrame 2\n",
    "valeur_min_x = dataframe2['xmin'][20]\n",
    "valeur_max_x = dataframe2['xmax'][20]\n",
    "valeur_min_y = dataframe2['ymin'][20]\n",
    "valeur_max_y = dataframe2['ymax'][20]\n",
    "\n",
    "\n",
    "# Filtrer les points dans le DataFrame 1 en fonction des coordonnées\n",
    "df_subset_centroid_max= dataframe1[(dataframe1['centroid-0'] >= valeur_min_x) & (dataframe1['centroid-0'] <= valeur_max_x) & (dataframe1['centroid-1'] >= valeur_min_y) & (dataframe1['centroid-1'] <= valeur_max_y)]\n",
    "\n",
    "\n",
    "#### CENTROIDES PRESENTS DANS LE TRAY MIN\n",
    "\n",
    "# Exemple de DataFrame 1\n",
    "dataframe1 = df_centroid\n",
    "\n",
    "# Exemple de DataFrame 2\n",
    "dataframe2 = df_tray_min\n",
    "\n",
    "# Récupérer les valeurs de la première ligne du DataFrame 2\n",
    "valeur_min_x = dataframe2['xmin'][20]\n",
    "valeur_max_x = dataframe2['xmax'][20]\n",
    "valeur_min_y = dataframe2['ymin'][20]\n",
    "valeur_max_y = dataframe2['ymax'][20]\n",
    "\n",
    "\n",
    "# Filtrer les points dans le DataFrame 1 en fonction des coordonnées\n",
    "df_subset_centroid_min= dataframe1[(dataframe1['centroid-0'] >= valeur_min_x) & (dataframe1['centroid-0'] <= valeur_max_x) & (dataframe1['centroid-1'] >= valeur_min_y) & (dataframe1['centroid-1'] <= valeur_max_y)]\n",
    "\n",
    "\n",
    "####CENTROIDES PRESENTS DANS LA ZONE D'EXCLUSION\n",
    "df_subset_centroid_rejected= df_subset_centroid_max - df_subset_centroid_min\n",
    "\n",
    "###ON SUPPRIME LES OBJETS SITUES DANS LA ZONE D4EXCLUSION\n",
    "resultat = df_centroid[~df_centroid['label'].isin(df_subset_centroid_rejected['label'])]\n",
    "\n",
    "print(df_subset_centroid_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_mean_coords = []\n",
    "y_mean_coords = []\n",
    "labels = []\n",
    "df_filtered_reinit=df_filtered.reset_index(drop=True)\n",
    "\n",
    "for k in range(len(df_filtered_reinit)):\n",
    "    data_coords = df_filtered_reinit['coords'][k]\n",
    "    coords = data_coords.tolist()\n",
    "    x_coords = coords[::2]\n",
    "    y_coords = coords[1::2]\n",
    "\n",
    "    mean_x = min(x_coords)\n",
    "    mean_y = min(y_coords)\n",
    "\n",
    "    x_mean_coords.append(mean_x)\n",
    "    y_mean_coords.append(mean_y)\n",
    "    labels.append(df_filtered_reinit['label'][k])\n",
    "\n",
    "data_mean_pixel_coord = {'label': labels,\n",
    "                         'x_mean': x_mean_coords,\n",
    "                         'y_mean': y_mean_coords}\n",
    "\n",
    "df_mean_pixel_coord = pd.DataFrame(data_mean_pixel_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "\n",
    "# Charger l'image\n",
    "image = filtered_label_image\n",
    "\n",
    "# Créer la figure et les axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Afficher l'image\n",
    "ax.imshow(image)\n",
    "\n",
    "centrioles_x = df_mean_pixel_coord ['x_mean'].tolist()\n",
    "centrioles_y = df_mean_pixel_coord ['y_mean'].tolist()\n",
    "\n",
    "# Tracer les centrioles\n",
    "for x, y in zip(centrioles_x, centrioles_y):\n",
    "    circle = patches.Circle((x, y), radius=10, color='red')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "# Afficher le tracé\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pot segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Charger l'image\n",
    "image = imread('/home/mcaroulle/Pheno_Leaf/Data/Color segmentation/TRAY_1.png')\n",
    "\n",
    "# Extraire les dimensions de l'image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "#Calcul de la largeur et de la hauteur des pots dans le tray\n",
    "x_pot = width/7\n",
    "y_pot = height/5\n",
    "\n",
    "\n",
    "# Création des données qui correspond à un découpage du tray en pots\n",
    "x_min=[]\n",
    "x_max=[]\n",
    "X=[]\n",
    "\n",
    "for k in range(5):\n",
    "    x=0\n",
    "    for k in range (8):\n",
    "        X.append(int(x))\n",
    "        x+=x_pot\n",
    "    x_min+=X[0:7]\n",
    "    x_max+=X[1:8]\n",
    "    if x_max[-1]!=width:\n",
    "        x_max[-1]=width\n",
    "\n",
    "\n",
    "y_min=[]\n",
    "y_max=[]\n",
    "Y=[]\n",
    "\n",
    "\n",
    "for k in range(7):\n",
    "    y=0\n",
    "    for k in range (6):\n",
    "        Y.append(int(y))\n",
    "        y+=y_pot\n",
    "    y_min+=Y[0:5]\n",
    "    y_max+=Y[1:6]\n",
    "    if y_max[-1]!=height:\n",
    "        y_max[-1]=height\n",
    "\n",
    "\n",
    "data_tray_max = {'Pot_position': [i for i in range(1, 36)],\n",
    "        'xmin': [k for k in x_min],\n",
    "        'xmax': [k for k in x_max],\n",
    "        'ymin': [k for k in y_min],\n",
    "        'ymax': [k for k in y_max]}\n",
    "\n",
    "df_tray_max = pd.DataFrame(data_tray_max)\n",
    "#print(df_tray_max)\n",
    "\n",
    "\n",
    "\n",
    "##création du tableau ne contenant que les centroïdes en filtrant le dataframe du masque obtenu précemment\n",
    "df_centroid = df_filtered [['label','centroid-0','centroid-1']]\n",
    "\n",
    "# on crée un df vide pour recueillir les résultats\n",
    "data_resultat = {'label': [],\n",
    "        'centroid-0': [],\n",
    "        'centroid-1': []}\n",
    "\n",
    "df_resultat = pd.DataFrame(data_resultat)\n",
    "\n",
    "\n",
    "## Fusion des labels \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "merged_label_image = np.copy(label_image)\n",
    "\n",
    "for k in range (len(df_tray_max)):\n",
    "\n",
    "    ##### CENTROIDES PRESENTS DANS TRAY MAX\n",
    "\n",
    "    df_subset_centroid_max= df_centroid[(df_centroid['centroid-0'] >= df_tray_max['xmin'][k]) & (df_centroid['centroid-0'] <= df_tray_max['xmax'][k]) & (df_centroid['centroid-1'] >= df_tray_max['ymin'][k]) & (df_centroid['centroid-1'] <= df_tray_max['ymax'][k])]\n",
    "\n",
    "\n",
    "    ### Fusion des labels des centroides présents\n",
    "    #function wich fuse all the labels contained in a list\n",
    "\n",
    "    list_labels =  df_subset_centroid_max['label'].tolist()\n",
    "    \n",
    "    for k in range (len(list_labels)-1):\n",
    "        merged_label_image[merged_label_image == list_labels[k+1]] = list_labels[k]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "image_label_overlay = label2rgb(merged_label_image, image=image)\n",
    "plt.imshow(image_label_overlay)\n",
    "\n",
    "\n",
    "###ON SUPPRIME LES OBJETS SITUES DANS LA ZONE D4EXCLUSION\n",
    "#resultat += df_centroid[~df_centroid['label'].isin(df_subset_centroid_rejected['label'])]\n",
    "\n",
    "\n",
    "##print(df_subset_centroid_rejected)\n",
    "\n",
    "#df_final = df_filtered[~df_filtered['label'].isin(df_subset_centroid_rejected['label'])]\n",
    "\n",
    "# Affichage\n",
    "#on_y_est = np.isin(cleared_label_image , df_final['label'])\n",
    "\n",
    "\n",
    "# Show the filtered label_image\n",
    "#plt.imshow(on_y_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "from skimage import color\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "###On définit une fonction qui permet de convertir un masque en une image\n",
    "def convertir_masque_en_image(mask):\n",
    "    # Créer une image vide avec les mêmes dimensions que le masque\n",
    "    image = np.zeros_like(mask)\n",
    "\n",
    "    # Appliquer le masque à l'image en définissant les pixels blancs\n",
    "    image[mask > 0] = 255\n",
    "\n",
    "    # Enregistrer l'image\n",
    "    return image\n",
    "\n",
    "image = convertir_masque_en_image(closed_mask)\n",
    "\n",
    "edges = feature.canny(image, sigma=10)\n",
    "\n",
    "dt = distance_transform_edt(~edges)\n",
    "\n",
    "local_max = feature.peak_local_max(dt, min_distance=5)\n",
    "\n",
    "plt.imshow(local_max, cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhenoLeaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
